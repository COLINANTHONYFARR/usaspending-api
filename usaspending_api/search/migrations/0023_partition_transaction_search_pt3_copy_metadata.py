# Generated by Django 3.2.15 on 2023-02-17 17:22
import logging
from pprint import pformat

from django.core.management import call_command
from django.db import migrations, connection

from usaspending_api.etl.broker_etl_helpers import dictfetchall
from usaspending_api.etl.management.commands.copy_table_metadata import make_read_indexes, make_read_constraints
from usaspending_api.common.helpers.sql_helpers import get_parent_partitioned_table

_index_template_suffix = "index_template"

logger = logging.getLogger(__name__)


def drop_metadata(target_table, is_potential_partition=False):
    table_name = target_table
    if "." in target_table:
        schema_name, table_name = target_table[: target_table.index(".")], target_table[target_table.index(".") + 1 :]
    else:
        schema_name = "public"
    with connection.cursor() as cursor:
        # Check if the table exists first
        cursor.execute(
            f"""
            SELECT EXISTS (SELECT FROM pg_tables 
            WHERE schemaname = '{schema_name}' AND tablename = '{table_name}');
        """
        )
        exists_results = cursor.fetchone()
        if not exists_results[0]:
            return

        if is_potential_partition:
            # Check that it is attached to a parent
            parent_partitioned_table = get_parent_partitioned_table(f"{schema_name}.{table_name}", cursor)
            if parent_partitioned_table:
                logger.info(
                    f"First detaching table '{target_table}' (a partition) from parent table "
                    f"'{parent_partitioned_table}' before dropping metadata on it."
                )
                cursor.execute(f"ALTER TABLE {parent_partitioned_table} DETACH PARTITION {target_table}")
            else:
                logger.info(f"'{target_table} is not currently attached to a parent as a partition")

        drop_constr_sql = make_drop_constraints(cursor, target_table)
        if drop_constr_sql:
            cursor.execute("; ".join(drop_constr_sql))
        drop_idx_sql = make_drop_indexes(cursor, target_table)
        if drop_idx_sql:
            cursor.execute("; ".join(drop_idx_sql))


def make_drop_constraints(cursor, target_table, drop_foreign_keys=False):
    # read the existing constraints
    cursor.execute(make_read_constraints(target_table)[0])
    src_constrs = dictfetchall(cursor)

    # build the destination constraint sql
    drop_constr_sql = []
    for src_constr_dict in src_constrs:
        src_constr_name = src_constr_dict["conname"]
        create_constr_content = src_constr_dict["pg_get_constraintdef"]
        if "FOREIGN KEY" in create_constr_content and drop_foreign_keys:
            continue
        drop_constr_sql.append(f"ALTER TABLE {target_table} DROP CONSTRAINT IF EXISTS {src_constr_name}")
    return drop_constr_sql


def make_drop_indexes(cursor, target_table):
    if "." in target_table:
        schema_name, table_name = target_table[: target_table.index(".")], target_table[target_table.index(".") + 1 :]
    else:
        schema_name = "public"

    # read the existing indexes of source table
    cursor.execute(make_read_indexes(target_table)[0])
    src_indexes = dictfetchall(cursor)

    # build the drop index sql
    drop_ix_sql = []
    for src_ix_dict in src_indexes:
        src_ix_name = src_ix_dict["indexname"]
        drop_ix_sql.append(f"DROP INDEX IF EXISTS {schema_name}.{src_ix_name}")
    return drop_ix_sql


def attach_child_partition_metadata(parent_partition_table, child_partition_suffix):
    # read the existing indexes
    with connection.cursor() as cursor:
        if "." in parent_partition_table:
            schema_name, table_name = (
                parent_partition_table[: parent_partition_table.index(".")],
                parent_partition_table[parent_partition_table.index(".") + 1 :],
            )
        else:
            schema_name = "public"

        # read the existing indexes of parent partition table
        cursor.execute(make_read_indexes(parent_partition_table)[0])
        src_indexes = dictfetchall(cursor)

        # build the attach partition index sql
        attach_ix_sql = []
        for src_ix_dict in src_indexes:
            src_ix_name = src_ix_dict["indexname"]
            attach_ix_sql.append(
                f"ALTER INDEX {schema_name}.{src_ix_name} "
                f"ATTACH PARTITION {schema_name}.{src_ix_name[:-5]}{child_partition_suffix}_temp"
            )
        logger.info(f"Running SQL to ATTACH child partition indexes to parent:\n{pformat(attach_ix_sql)}")
        cursor.execute("; ".join(attach_ix_sql))


class Migration(migrations.Migration):
    # Subsequent steps use a different connection than the one running migrations, so in order to see tables or DB
    # objects being created or dropped by prior steps, the migration steps need to be made non-atomic
    atomic = False

    dependencies = [
        ("search", "0022_partition_transaction_search_pt2_load_data"),
    ]

    operations = [
        # STEP 1: Copy all template metadata on to the partition table, as "virtual" indexes/constraints
        #         Using the --only-parent-partitioned-table flag will apply them ONLY to the parent partition table
        #         Then also apply them directly to each partition, and finally ATTACH them to the parent metadata
        migrations.RunPython(
            code=lambda apps, _: call_command(
                "copy_table_metadata",
                f"--source-table=temp.transaction_search_{_index_template_suffix}",
                "--dest-table=temp.transaction_search_temp",
                "--dest-suffix=temp",
                f"--source-suffix={_index_template_suffix}",
                "--only-parent-partitioned-table",
                "--index-concurrency=15",
            ),
            reverse_code=lambda apps, _: drop_metadata("temp.transaction_search_temp"),
        ),
        migrations.RunPython(
            code=lambda apps, _: call_command(
                "copy_table_metadata",
                f"--source-table=temp.transaction_search_{_index_template_suffix}",
                "--dest-table=temp.transaction_search_fpds_temp",
                f"--source-suffix={_index_template_suffix}",
                "--dest-suffix=fpds_temp",
                "--index-concurrency=15",
            ),
            reverse_code=lambda apps, _: drop_metadata(
                "temp.transaction_search_fpds_temp", is_potential_partition=True
            ),
        ),
        migrations.RunPython(
            code=lambda apps, _: call_command(
                "copy_table_metadata",
                f"--source-table=temp.transaction_search_{_index_template_suffix}",
                "--dest-table=temp.transaction_search_fabs_temp",
                f"--source-suffix={_index_template_suffix}",
                "--dest-suffix=fabs_temp",
                "--index-concurrency=15",
            ),
            reverse_code=lambda apps, _: drop_metadata(
                "temp.transaction_search_fabs_temp", is_potential_partition=True
            ),
        ),
        migrations.RunPython(
            code=lambda apps, _: attach_child_partition_metadata(
                parent_partition_table="temp.transaction_search_temp", child_partition_suffix="_fabs"
            ),
            reverse_code=migrations.RunPython.noop,
        ),
        migrations.RunPython(
            code=lambda apps, _: attach_child_partition_metadata(
                parent_partition_table="temp.transaction_search_temp", child_partition_suffix="_fpds"
            ),
            reverse_code=migrations.RunPython.noop,
        ),

        # STEP 2: Create empty/placeholder partitions for the to-be partitioned rpt.transaction_search in the rpt schema
        migrations.RunSQL(
            sql=f"""
                CREATE TABLE rpt.transaction_search_fabs (LIKE temp.transaction_search_fabs_temp);
                CREATE TABLE rpt.transaction_search_fpds (LIKE temp.transaction_search_fpds_temp);
            """,
            reverse_sql=f"""
                DROP TABLE IF EXISTS rpt.transaction_search_fabs;
                DROP TABLE IF EXISTS rpt.transaction_search_fpds;
            """,
        ),

        # STEP 3: Copy the metadata from the temp partitions to these new placeholder partitions, so that their
        # metadata matches (except for _temp suffix on the source names), which will allow a swap to take place
        migrations.RunPython(
            code=lambda apps, _: call_command(
                "copy_table_metadata",
                f"--source-table=temp.transaction_search_fabs_temp",
                "--dest-table=rpt.transaction_search_fabs",
                f"--source-suffix=temp",
                "--dest-suffix",
                "--index-concurrency=15",
            ),
            reverse_code=lambda apps, _: drop_metadata("rpt.transaction_search_fabs", is_potential_partition=True),
        ),
        migrations.RunPython(
            code=lambda apps, _: call_command(
                "copy_table_metadata",
                f"--source-table=temp.transaction_search_fpds_temp",
                "--dest-table=rpt.transaction_search_fpds",
                f"--source-suffix=temp",
                "--dest-suffix",
                "--index-concurrency=15",
            ),
            reverse_code=lambda apps, _: drop_metadata("rpt.transaction_search_fpds", is_potential_partition=True),
        ),
    ]
